---
title: "SMekala_Final_Exam"
author: "Sekhar Mekala"
date: "Saturday, December 12, 2015"
output: html_document
---
#1. Essential concepts

##Problem-1
Given the following matrix:

$$\left[ \begin{array}{cccc} 1 & -1 & 3 & -5 \\ 2 & 1 & 5 & -9 \\ 6 & -1 & -2 & 4 \end{array} \right]$$

Since we have at least one non-zero element the rank of this matrix is at least 1. This is a 3X4 matrix, so the maximum rank of this matrix is 3 (minimum of "number of rows"" and "number of columns"). To identify the actual rank of the given matrix, we need to reduce the matrix to row echelon form. This way we can find the number of linerarly indepenednt rows and thus get the rank of the matrix.

Using elementary row operations, to get the row echelon form of the matrix:

$$\left[ \begin{array}{cccc} 1 & -1 & 3 & -5 \\ 2 & 1 & 5 & -9 \\ 6 & -1 & -2 & 4 \end{array} \right]$$

Applying $R_{2} - 2R_{1}$ and $R_{3} - 6R_{1}$:

$$\left[ \begin{array}{cccc} 1 & -1 & 3 & -5 \\ 0 & 3 & -1 & 1 \\ 0 & 5 & -20 & 34 \end{array} \right]$$

Applying $R_{3} - 5R_{2}$:

$$\left[ \begin{array}{cccc} 1 & -1 & 3 & -5 \\ 0 & 3 & -1 & 1 \\ 0 & 0 & -55 & 97 \end{array} \right]$$

Applying $\frac {R_{2}}{3}$ and $\frac {R_{3}}{-55}$:

$$\left[ \begin{array}{cccc} 1 & -1 & 3 & -5 \\ 0 & 1 & \frac {-1}{3} & \frac {1}{3} \\ 0 & 0 & 1 & \frac {-97}{55} \end{array} \right]$$

From the above matrix (which is in row echelon form), we can find that there are 3 linearly independent rows. 
Hence the **rank of the given matrix is 3**


##Problem-2

The transpose of the above matrix is:

$$\left[ \begin{array}{ccc} 1 & 2 & 6 \\ -1 & 1 & -1 \\ 3 & 5 & -2 \\ -5 & -9 & 4 \end{array} \right]$$

The following R code can be used to get the transpose of the matrix:

```{r}
a <- matrix(c(1,-1,3,-5,2,1,5,-9,6,-1,-2,4),byrow=T, nrow = 3)
t(a)
```


##Problem-3

A set of vectors $\{\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, \overrightarrow{v_{3}} ... \overrightarrow{v_{n}}\}$ are said to be orthonormal if all these vectors have a magnitude of 1, all these vectors are perpendicular to each other and all these vectors are independent to each other. If all the vectors in the Euclidean space $\mathbb{R}^n$ can be expressed using a scalar and a set of orthonormal vectors, then such orthonormal vectors set are said to be the basis for $\mathbb{R}^n$. Such basis is called orthonormal basis. For $\mathbb{R}^3$ space, we can use the following vectors to express any vector in $\mathbb{R}^3$:

$\left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 1 \\ 0 \end{array} \right], \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right]$ 

These 3 vectors can be used to express any vector in the 3D space (or $\mathbb{R}^3$)


##Problem-4

Given that

$$A = \left[ \begin{array}{cccc} 5 & 0 & 3 \\ 0 & 1 & -2 \\ 1 & 2 & 0 \end{array} \right]$$

To find the characteristic polynomial, we have express the following:

$$Av = \lambda v$$

Where A is the given matrix, **v** is the eigen vector, and $\lambda$ is an eigen value of **v**


$Av = \lambda v$

==> $Av = \lambda I v$ 

==> $\lambda I v - Av = 0$ 

==> $(\lambda I - A)v = 0$

In the above equation, **v** cannot be a zero vetor. Hence $(\lambda I - A)$ must be singular. Since $(\lambda I - A)$ is singular, the $|(\lambda I - A)|$ must be zero.

Therefore,

$|(\lambda I - A)| = 0$

==> $\left|\left[\begin{array}{ccc} \lambda & 0 & 0 \\ 0 & \lambda & 0 \\ 0 & 0 & \lambda \end{array}\right] - \left[ \begin{array}{cccc} 5 & 0 & 3 \\ 0 & 1 & -2 \\ 1 & 2 & 0 \end{array} \right]\right| = 0$

Hence we obtain the characteristic polynomial as 
$$\lambda^3 - 6\lambda^2 + 6\lambda - 17 = 0$$

##Problem-5

To find the eigen values and eigen vectors of the characteristic polynomial obtained (in the previous problem), we have to solve the obtained characteristic (I mean finding the roots of) $$\lambda^3 - 6\lambda^2 + 6\lambda - 17 = 0$$.

There is no straingt forward method to find the roots of a ploynomial (of more than 2 degrees). We will first plot the characteristic polynomial function obtained above in the interval [-5,5] of $\lambda$

```{r}
curve(x^3 - 6*x^2 + 6*x - 17,from=-10,to=10,type="l")
#abline(h=0,v=10,col=10)
abline(h=0,col=10)
```

The above plot shows that the one of the roots of the polynomial is around 5 and 6. We will use Newton's method to find the specific value.

To use newton's method, we need to obtain the derivative of $f(\lambda) = \lambda^3 - 6\lambda^2 + 6\lambda - 17$, which is nothing but 
$$f'(\lambda) = 3\lambda^2 - 12\lambda + 6$$

The following R code will use the Newton's method to find the root:

```{r}
x0 <- 6; #We will use 6, since as per the graph one of the root should lie between 5 and 6
i <- 0
#We will stop the iteration when we get an error of <= 0.00001 or after repeating the process for 100 times

repeat{
fun_at_x0 <- x0^3 -6*x0^2 + 6*x0 - 17
der_at_x0 <- 3 * x0 - 12 * x0 + 6
x0 <- x0 - abs(fun_at_x0/der_at_x0)
if(abs(fun_at_x0) <= 0.00001) break

if(i > 100) break

i <- i+1

}

print(paste("One of the roots of the equation is:", x0))
```

Hence we obtained one of the roots of the equation as $\lambda^3 - 6\lambda^2 + 6\lambda - 17 = 0$ as 5.47126358577092.
The following R code will verify this value:

```{r}
x0^3 - 6*x0^2 + 6*x0 - 17

```
The above value is almost zero, and hence 5.471264 is one of the roots. 

Let us plot the graph again, along with the lines where the function is zero (with the value of $\lambda=5.471264$):

```{r}
curve(x^3 - 6*x^2 + 6*x - 17,from=-10,to=10,type="l")
abline(h=0,v=5.471264,col=10)
```

Let us find the other roots of $\lambda^3 - 6\lambda^2 + 6\lambda - 17 = 0$. We will divide this equation with $\lambda - 5.471264$. We will obtain the following expression (by factoring out $\lambda - 5.471264$):


$\lambda^3 - 6\lambda^2 + 6\lambda - 17 = (\lambda - 5.471264)(\lambda^2 - 0.528736\lambda + 3.107146)$

Solving $\lambda^2 - 0.528736\lambda + 3.107146=0$ will get us

$\lambda = 0.264368 \pm 1.742772i$

Hence the eigen values of A are:
5.471264, 0.264368+1.742772i and 0.264368-1.742772i

Using these eigen values, we can obtain the eigen vectors as (actually I used $eigen(A)$ function of R to obtain the following vectors):

$\left[ \begin{array}{c} 0.98551404 \\ -0.06924766 \\ 0.1548122\end{array} \right]$

$\left[ \begin{array}{c} 0.2583505-0.2761849i \\ -0.6725516 \\ -0.2473752+0.5860519i\end{array} \right]$

$\left[ \begin{array}{c} 0.2583505+0.2761849i \\ -0.6725516 \\ -0.2473752-0.5860519i\end{array} \right]$

##Problem-6

If we have a column stochastic matrix representing the links between URLs, then sum of the elements of a column will be equal to 1. Also all the elements of such matrix must be greater than or equal to zero. 

If such matrix (say A) is multiplied by itself (say for n iterations) in such a way that we get $A^n r = r$, where r is the rank matrix, then r represents the ranks of the URLs (also this r is the eigen vector corresponding to the eigen value of 1 for $A^n$)


##Problem-7
If a set of numbers are repeatedly sampled with replacement from a population, then the averages of such sets form a normal distribution (irrespective of the probability distribution of the population), and the average of the averages (or mean of the obtained normal distribution, after obtaning a sufficiently large number of samples) represents the true population mean.

##Problem-8

$\frac {d}{dx}(e^x cos^2(x)) = e^x cos^2(x) + e^x 2 cos(x) (-sin(x))$
==> $e^x cos^2(x) - 2 e^x cos(x) sin(x)$

##Problem-9
$\frac {d}{dx}(e^{x^3}) = e^{x^3}  3x^2 = 3x^2 e^{x^3}$

##Problem-10

${\displaystyle \int e^x cos(x) + sin(x)dx = \int e^x cos(x) dx + \int sin(x) dx}$

Let U = $\int e^x cos(x) dx$

Integrating by parts.

U = $\int e^x cos(x) dx = cos(x) e^x - \int (-sin(x)) e^x dx = cos(x) e^x + \int (sin(x)) e^x dx = cos(x) e^x + sin(x) e^x - \int (cos(x)) e^x dx$

But U = $\int e^x cos(x) dx$

Therefore,

U = $cos(x) e^x + sin(x) e^x - \int (cos(x)) e^x dx = cos(x) e^x + sin(x) e^x - U$
==> U = $\frac{cos(x) e^x + sin(x) e^x}{2} + C$

Finally,

${\displaystyle \int e^x cos(x) + sin(x)dx = \int e^x cos(x) dx + \int sin(x) dx = \frac{cos(x) e^x + sin(x) e^x}{2}- cos(x) + C}$


#2. Mini-coding assignments

##Problem-2.1(Sampling from function)

We can use an in-built R function (rbinom) to generate random numbers from a binomial distribution. But I will be implementing my own random value generator from a binomial distribution. We wil compare the results of my random number generator with rbinom() function (the built in R function)

We will create 2 function p.binom(n,p), which generates the cumulative binomial distribution. This function will accept n and p as parameters. The "n" means the maximum value in the sample space {0,1,2,...n}. "p" is the probability of success. The function returns a vector containing the cumulative probabilities of {0,1,2,...n} (in the order 0,1,2...n). The other function, r.binom(x,n,p) is my function that returns a set of random values based on a binomial distribution (In this r.binom(x,n,p) function, x = Desired number of random numbers).


```{r}
options(digits=10)

p.binom <- function(n,p)
  {
  x <- vector(length=n+1)
  q <- 1 - p
  x[1] <- q^n
  for(i in 1:n){
      x[i+1] <- x[i] + ((factorial(n)/(factorial(n-i) * factorial(i))) * p^i * q^(n-i))
    }
  return(x)
  }
```

Now writing the actual random number generator for binomial distribution:

```{r}
r.binom <- function(x, n, p)
  {
  #x - Desired number of random numbers to be generated

  
U <- runif(x)
r <- vector(length=length(x))

cumulative.p <- p.binom(n, p)

for(i in 1:length(U))
  {
  if(U[i] < cumulative.p[1]) {
    r[i] <- 0
    next
  }
  else{
    int <- which(cumulative.p <= U[i])
    r[i] <- int[length(int)]
  }

  }
return(r)
  }
```


Let us generate 1000 random numbers from the binomial distribution (n = 20 and p = 0.25), and plot a histogram:

```{r}
set.seed(1)
r1 <- r.binom(1000,20,0.25)
hist(r1, main="Random numbers generated by the function we created (r.binom())")
```

We will verify, if our function r.binom() is operating correctly, by comparing its output with rbinom() function (built in function in R), using the following code:

```{r}
set.seed(1)
r2 <- rbinom(1000,20,0.25)
hist(r2, main="Random numbers generated by built-in function rbinom()")
print(all((r1-r2)==0))
```

The histograms are identical, and the two random number vectors have the same values (see the last statement in the above code)

##Problem-2.2(Principal Component Analysis)

Reading the file into a data frame ("df"):

```{r}

#setwd("C:/Users/Sekhar/Documents/R Programs/MSDA 605 - Comp methods/IS605_Finals_Fall2015/IS605_Finals_Fall2015")
df <- read.csv("auto-mpg.data", header = F, sep = "")
head(df)
nrow(df)
names(df) <- c("displacement", "horsepower", "weight", "acceleration","mpg")
df <- df[,-5]
head(df)
```
Scaling the data frame. This will make the mean as 0 and standard deviation of 1:

```{r}
df_scaled <- apply(df,2,scale)
```


Applying the singular value decomposition function (svd):

```{r}
svd <- svd(df_scaled)
pc_matrix <- svd$v
e <- svd$d

e1 <- e[1]^2/sum(e^2)
e2 <- e[2]^2/sum(e^2)
e3 <- e[3]^2/sum(e^2)
e4 <- e[4]^2/sum(e^2)

plot(1:4,c(e1,e2,e3,e4),xaxt='n',type="b",main = "Variance explained by eigen values",xlab="Eigen values",ylab="Variance Explained")
axis(side=1,at=c(1,2,3,4),labels=TRUE)

data.frame(e1,e2,e3,e4)


```

We can easily confirm that the first two eigen values explains the maximum variance (of approximately 96.35%). So will consider only the first two eigen vectors (first two when ordered by the eigen values in descending order).

Transforming the dimensions of the data to the PC1 and PC2 (principal components 1 and 2):

```{r}
rownames(pc_matrix) <- c("displacement","horsepower", "weight", "acceleration")
colnames(pc_matrix) <- c("PC1","PC2","PC3","PC4")
pc_matrix

PC1_Points <- df_scaled %*% pc_matrix[,"PC1"]
PC2_Points <- df_scaled %*% pc_matrix[,"PC2"]
```
Let us plot the transformed data to PC1 and PC2 as axis:

```{r}
plot(PC1_Points, PC2_Points,col='blue',cex=0.4,lty='solid',lwd=2, main="Data transformed to PC1 and PC2",xlab="PC1",ylab="PC2")
abline(h=0,v=0,col="gray60")

#Plot the dimensional vectors 
arrows(0,0,pc_matrix["displacement",1],pc_matrix["displacement",2],col="red",lwd=2)
text(pc_matrix["displacement",1],pc_matrix["displacement",2],label="Displacement",col="red")

arrows(0,0,pc_matrix["horsepower",1],pc_matrix["horsepower",2],col="red",lwd=2)
text(pc_matrix["horsepower",1],pc_matrix["horsepower",2],label="Horsepower",col="red")

arrows(0,0,pc_matrix["weight",1],pc_matrix["weight",2],col="red",lwd=2)
text(pc_matrix["weight",1],pc_matrix["weight",2],label="Weight",col="red")

arrows(0,0,pc_matrix["acceleration",1],pc_matrix["acceleration",2],col="red",lwd=2)
text(pc_matrix["acceleration",1],pc_matrix["acceleration",2],label="Acceleration",col="red")

```

Another plot (same as above, but with observation numbers)

```{r}

plot(PC1_Points, PC2_Points,col='blue',cex=0.4,lty='solid',lwd=2, main="Data transformed to PC1 and PC2 with observation numbers",xlab="PC1",ylab="PC2")
abline(h=0,v=0,col="gray60")

#Plot the dimensional vectors 
arrows(0,0,pc_matrix["displacement",1],pc_matrix["displacement",2],col="red",lwd=2)
text(pc_matrix["displacement",1],pc_matrix["displacement",2],label="Displacement",col="red")

arrows(0,0,pc_matrix["horsepower",1],pc_matrix["horsepower",2],col="red",lwd=2)
text(pc_matrix["horsepower",1],pc_matrix["horsepower",2],label="Horsepower",col="red")

arrows(0,0,pc_matrix["weight",1],pc_matrix["weight",2],col="red",lwd=2)
text(pc_matrix["weight",1],pc_matrix["weight",2],label="Weight",col="red")

arrows(0,0,pc_matrix["acceleration",1],pc_matrix["acceleration",2],col="red",lwd=2)
text(pc_matrix["acceleration",1],pc_matrix["acceleration",2],label="Acceleration",col="red")

#Plotting the observations
text(PC1_Points,PC2_Points,labels=1:nrow(PC2_Points))

```

###Analysis of the plot:

The "horsepower" dimension is mostly explained by PC1. PC1 also explains the variance in "weignt", "displacement" and "acceleration". But the acceleration is explained more in PC2 (see the span of the vectors plotted in red color).

We will now examine some of the observations:

Let us consider observation 298 and 202. As per the graph, the 298th observation should have more acceleration, more weight, more displacement, but similar horsepower (based on observation numbers plotted in the graph):

```{r}
df[298,]
df[202,]
```

Let us consider observation 95 and observation 217. Both these should have approximately same acceleration, but all other attribute values must be higher for the 95th observation. 

```{r}
df[95,]
df[217,]
```


##Problem-2.3(Sampling in bootstrapping)

Let us assume that we have $n$ number of elements in a population. In bootstrapping, We will repeatedly sample one element at a time with replacement, until we get $n$ samples. 

The probability that an element $x$ is selected = $P(n=x) = \frac{1}{n}$.
The probability that an element $x$ is NOT selected = $P(n \neq x) = 1 - \frac{1}{n}$

Let us suppose that $r$ be the proportion of $n$ elements that are never selected when we perform bootstrap sampling on $n$ elements. 

$$r = (1-1/n)^n = 1 + n.(\frac{-1}{n}) + \frac{n(n-1)}{2!}(\frac{-1}{n})^2 + \frac{n(n-1)(n-2)}{3!}(\frac{-1}{n})^3 + .... + (\frac{-1}{n})^n$$

If we have an infinite number of elements, (that is if $n \rightarrow \infty$), then we will have the following:

$$r = \lim_{n \rightarrow \infty}(1-1/n)^n = \lim_{n \rightarrow \infty}(1 + n.(\frac{-1}{n}) + \frac{n(n-1)}{2!}(\frac{-1}{n})^2 + \frac{n(n-1)(n-2)}{3!}(\frac{-1}{n})^3 + .... + (\frac{-1}{n})^n)$$

But as $n \rightarrow \infty$, all the expressions of the form $\frac{n(n-1)}{n^2}$, $\frac{n(n-1)(n-2)}{n^3}$ ... will become 1. Also $(\frac{-1}{n})^n$ will be zero.

Thus, 
$$r = \lim_{n \rightarrow \infty}(1-1/n)^n = 1 - 1 + \frac{1}{2!} - \frac{1}{3!} + \frac{1}{4!} - ....+0$$ 
But this nothing but the expansion of $e^{-1}$

Hence proportion of elements that are never selected in bootstrap (as number of elements tends to infinity) = $e^{-1} = \frac{1}{2.718} = 0.3679 = 0.368$

This implies that a proportion of $1-0.368 = 0.632$ (or 63.2%) elements are always selected (as the number of elements reaches infinity).

The following R code performs a simulation of bootstrap method. To make things simple, we will generate some $x$ numbers (1 to $x$) and randomly select 1 element at a time (for $x$ times), with repetitions. Then compare how many of the elements selected are present in the actual set of numbers.

```{r}

bootstrap <- function(x)
  {
  
  population <- seq(x)
  bootstrap <- sample(population,x,replace=TRUE)
  
  t <- length(setdiff(population,bootstrap))
  
  print(paste("For ",x," elements:"))
  print(paste("Percentage of elements selected:",100 - ((t/x)*100)))
  print(paste("Percentage of elements NOT selected:",((t/x)*100)))

  
  }
```

Let us execute the above function for 100, 1000, and 10000 number of elements.

```{r}
x<- 100
bootstrap(x)

x<- 1000
bootstrap(x)

x<- 10000
bootstrap(x)

```

The above results show that as the number of elements increase, the percentage of the elements selected will reach around 63.2%


#3. Mini-project

Reading the files:

```{r}


df_src <- read.csv("ex3x.dat", header = F, sep = "")
df_tgt <- read.csv("ex3y.dat", header = F, sep = "")

df <- data.frame(sft=df_src$V1,beds=df_src$V2,price=df_tgt$V1)
head(df)

```


Standardizing the data:

```{r}
df_std <- apply(df,2,scale)
df_std <- data.frame(df_std)
```

Let us use linear regression to predict the price of a home, using the square feet (sft) and number of bedrooms (beds)

$$price = \theta_{0} + \theta_{1} \mbox{ } sft + \theta_{2} \mbox{ } beds$$

We will assume $\theta_{0} = \theta_{1} = \theta_{2} = 1$ initially, and use the following correction, iteratively (for about 100 times), till the values of $\theta$ stablize.

$$\theta_{j} = \theta_{j} + \alpha \Sigma_{i=1}^{m} (y^{(i)} - h_{\theta}(x^{(i)}) )x_{j}^{(i)}$$

where $\theta$ is coefficients of our linear regression function, $y^{(i)}$ is the observed $price$ of an observation $i$, $h_{\theta}(x^{(i)}) )x_{j}$ is our linear regression function ($price = \theta_{0} + \theta_{1} \mbox{ } sft + \theta_{2} \mbox{ } beds$), and $x_{j}^{i}$ is the $j^{th}$ predictor variable of the $i^{th}$ observation.

Let us write a function that takes the $\alpha$ as input, and performs the gradient descent, to obtain the coefficients of the linear regression function. The function takes, alpha, data frame and number of iterations desired as input values, and returns coefficients and the error values at the end of each iteration. This function will process all the observations, before finding the value that has to be added to the current theta values (coefficients). This method of gradient descent is called batch gradient descent.


**Batch Gradient Descent function**
```{r}
batch_gradient_descent <- function(alpha,df_std,rep)
  {

  theta_0 <- 1
  theta_1 <- 1
  theta_2 <- 1
l <- nrow(df_std)
 
x_zero <- 1
x0_running_total <- 0
sft_running_total <- 0
beds_running_total <- 0

j_theta <- vector()
k <- 0

#performing 100 iterations
for(j in 1:rep)
{
for(i in 1:l)
{
  temp <- df_std$price[i] - (theta_0 + theta_1 * df_std$sft[i] + theta_2 * df_std$beds[i])
  x0_running_total <- x0_running_total +  temp * x_zero
 
  sft_running_total <- sft_running_total + temp * df_std$sft[i]
  beds_running_total <- beds_running_total + temp * df_std$beds[i]
}

theta_0 <- theta_0 + alpha * x0_running_total
theta_1 <- theta_1 + alpha * sft_running_total
theta_2 <- theta_2 + alpha * beds_running_total
 
x0_running_total <- 0
sft_running_total <- 0
beds_running_total <- 0

#Getting the error in prediction
#j_theta[j] = sum(df_std$price - as.matrix(cbind(1,df_std[,-3])) %*% matrix(rbind(theta_0,theta_1,theta_2)))
j_theta[j] = sum((df_std$price - (as.matrix(cbind(1,df_std[,-3])) %*% matrix(rbind(theta_0,theta_1,theta_2))))^2)/(2*l)
  
  }
return(list(j_theta=j_theta,theta_0=theta_0,theta_1=theta_1,theta_2=theta_2))

}
```


**Batch gradient descent with $\alpha = 0.001$, 100 iterations and plotting the $J({\theta})$.**

```{r}
bg <- batch_gradient_descent(0.001,df_std,100)
plot(bg$j_theta,xlab="Iteration",ylab="J Theta", main="Cost function for 100 iterations and alpha = 0.001")
cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
cat("Last 5 iterations error values", bg$j_theta[96:100])
```

It looks like the error function, $J({\theta})$ is stablizing around 0.132. But there is still a room to optimize this further, since the difference between the last two error values is not approximately zero. We will perform some more iterations (200 iterations), and check how the error function behaves:

```{r}
bg <- batch_gradient_descent(0.001,df_std,200)
plot(bg$j_theta,xlab="Iteration",ylab="J Theta", main="Cost function for 200 iterations and alpha = 0.001")
cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
cat("Last 5 iterations error values", bg$j_theta[196:200])

```

With 200 iterations, the difference between the last 2 error values is of the order $10^{-6}$. We can stop here or test with other learning rates, to identify a better model.

**Batch gradient descent with $\alpha = 0.01$, 100 iterations and plotting the $J({\theta})$.**

```{r}
bg <- batch_gradient_descent(0.01,df_std,100)
plot(bg$j_theta,xlab="Iteration",ylab="Error", main="Cost function for 100 iterations and alpha = 0.01")
cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
cat("Last 5 iterations error values", bg$j_theta[96:100])
```

From the plot, we can easily confirm that after 20 iterations, the error function stablized around 0.1306865. Also the last 5 iterations error values (out of 100 iterations) are the same, suggesting that this is the best possible optimization (or minimizatin of error function).


**Batch gradient descent with $\alpha = 0.1$, 100 iterations and plotting the $J({\theta})$.**

```{r}
bg <- batch_gradient_descent(0.1,df_std,100)
plot(bg$j_theta,xlab="Iteration",ylab="J Theta", main="cost function for 100 iterations and alpha = 0.1")
cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
cat("Last 5 iterations error values", bg$j_theta[96:100])
```

The error function is increasing (see the last 5 values of the error function in the 100 iterations).


**Batch gradient descent with $\alpha = 1$, 100 iterations and plotting the $J({\theta})$.**

```{r}
bg <- batch_gradient_descent(1,df_std,100)
plot(bg$j_theta,xlab="Iteration",ylab="J Theta", main="Cost function for 100 iterations and alpha = 1")
cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
cat("Last 5 iterations error values", bg$j_theta[96:100])
```

Here also the error vlaues became infinity after 80 iterations.

For the learning rates of 0.1 and 1, we are not able to minimize the error function, since our learning rate (or the descent ste size is big, and we jumped over the optimal value).

###Conclusion

We will use the $\alpha=0.01$, since this value has minimized the error function to a maximum extent, and hence will use the following coefficients for the linear regression:

```{r}
bg <- batch_gradient_descent(0.01,df_std,100)

cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2)
```

Ignoring the intercept value, since it is of the order $10^{-16}$, we obtain the following regression equation (on the standardized values of price, beds, and square feet):

$$price = 0.884766 squarefeet - 0.05317882 bedrooms$$

###Using built-in R function
Let us use the built-in R function to get the linear regression (again on the standardized data):

```{r}
lm(data=df_std,price~beds+sft)
```

Hence we obtained the same linear regression function using the gradient descent algorithm (with a learning rate of 0.01).

###Using Ordinary Least Squares method

The following R code will perform Ordinary Least Squares method to obteain linear regression function to predict the house price, based on number of bed rooms and square feet:


```{r}
A <- df_std[,-3]
B<-df_std[,3]
A <- as.matrix(A)
B <- as.matrix(B)
solve(t(A) %*% A) %*% (t(A) %*% B)
```

The Ordinary Least Squares method has given the following linear model:

$$price = 0.88476599 squarefeet - 0.05317882 bedrooms$$

This model is the same model we obtained using gradient descent method, and also using the built-in R function ("lm").

###Stochastic Gradient Descent

In *batch gradient descent* we correct our coefficients after processing all the observations. But this method's run time will increase as the number of observations increase. Another method called *stochastic gradient descent* will update the coefficients for each observation. The following R code will perform *stochastic gradient descent* on the house price data (standardized data). The main drawback of *stochastic gradient descent* is that the parameters values do not converge, and oscillate around the optimal values. As per Andrew Ng's tutorial, one way to fix this problem is to start with a lower learning rate, and decrease the rate as we process the observations.

In our data, we have just 47 observations. So to make the data more realistic, we will perform a bootstrap sampling. We will generate 1000 observations in each iteration (by sampling the 47 observations with repetitions). We will repeat this for 30 times, and at the end of each iteration, we compute the cost function. We will also decrease the learning rate by a factor of {1,2,3,...30}, after each of the 30 iterations.


```{r}
stochastic_gradient_descent <- function(alpha,df_std)
  {

  theta_0 <- 1
  theta_1 <- 1
  theta_2 <- 1
  
  
l <- nrow(df_std)
k <- 1
#Shuffle the data


x_zero <- 1
x0_running_total <- 0
sft_running_total <- 0
beds_running_total <- 0
running_err <- 0
j_theta <- vector()

for(j in 1:30)
{
  set.seed(10)
  shuffle <- sample(1:l,1000,rep=TRUE)
for(i in shuffle)
{
  temp <- df_std$price[i] - (theta_0 + theta_1 * df_std$sft[i] + theta_2 * df_std$beds[i])
  x0_running_total <-   temp * x_zero
  sft_running_total <-  temp * df_std$sft[i]
  beds_running_total <- temp * df_std$beds[i]
  theta_0 <- theta_0 + alpha * x0_running_total
  theta_1 <- theta_1 + alpha * sft_running_total
  theta_2 <- theta_2 + alpha * beds_running_total
  
  #Getting the error in prediction

running_err <- (sum((df_std$price[i] - (theta_0+df_std$sft[i]*theta_1 + df_std$beds[i]*theta_2))^2)/2) + running_err
 }  
#j_theta[j] = sum((df_std$price - (as.matrix(cbind(1,df_std[,-3])) %*% matrix(rbind(theta_0,theta_1,theta_2))))^2)/(2)  
j_theta[j] = running_err/1000
running_err <- 0
alpha <- alpha/j
  
}

return(list(j_theta=j_theta,theta_0=theta_0,theta_1=theta_1,theta_2=theta_2))

}
```


Calling the stochastic gradient descent method (with an initial learning rate, alpha = 0.003333333):

```{r}

sg <- stochastic_gradient_descent(0.003333333,df_std)
plot(sg$j_theta,xlab="Iteration",ylab="J Theta", main="Cost function in stochastic gradient descent method")
cat("intercept = ", sg$theta_0, "    sft coefficient = ", sg$theta_1, "    beds coefficient = ", sg$theta_2)
cat("Last 5 iterations error values", sg$j_theta[26:30])
```

So we obtained the following linear model using the stochastic method:
$$price = -0.009845125+0.8683112 squarefeet - 0.05612595 beds$$


##Cross validation

We will first use the built-in cross validation R function to check which polynomial degree is optimal for the given data.

```{r}
library("boot")
cv.err10 = rep(0,10)
for (i in 1:12){
#set.seed(10)
glm.fit = glm(price~poly(sft + beds ,i),data=df_std)
cv.err10[i]=cv.glm(df_std,glm.fit,K=5)$delta[1]
}

degree=1:12
plot(degree,cv.err10,type='b',xlab = "Polynomial Degree", ylab = "Cross validation error", main = "Optimal polynomial degree selection")
```

The above plot shows that a linear model (polynomial model of degree 1) is an optimal model for the given data, since its cross validation error is minimal. So we can safely use the linear regression functions obtained by any of the methods used above. We will perform the cross validation of batch gradient and stochastic gradient descent methods, and evaluate if stochastic method can be used in the place of batch gradient descent (for the given data).

###Cross validation of Gradient Descent (batch method)

We will perform 5 fold cross validation on the linear regression function obtained by batch gradient descent. We will keep 5 observations from the data set as validation data and the remaining data as the training. We will use training data to build the model. We will evaluate the model's prediction using the validation data (the 5 observations). We will repeat this process on all the data, and find the mean squared error in each instance.


```{r}
i<-1
j<-5

l <- nrow(df_std)
MSE_batch <- vector()
k <- 1
repeat{

if(j > l) break

include <- seq(i,j,1)
exclude <- -1*include
bg <- batch_gradient_descent(0.01,df_std[exclude,],100)
#lm <- lm(data=df_std[exclude,],price~(sft+beds))
MSE_batch[k] <- sum((df_std[include,]$price - (bg$theta_0+bg$theta_1*df_std[include,]$sft+bg$theta_2*df_std[include,]$beds))^2)/5
k<- (k+1)
#print(cat("intercept = ", bg$theta_0, "    sft coefficient = ", bg$theta_1, "    beds coefficient = ", bg$theta_2))
#print(cat("Last 5 iterations error values", bg$j_theta[96:100]))

#sum((df_std[include,]$price - predict(lm,df_std[include,]))^2)/5
i <- i+5
j<- j+5

}
```

Let us plot the MSE.

```{r}
plot(MSE_batch,type="l",xlab="CV iteration",main="Cross Validation of batch gradient descent")
boxplot(MSE_batch, main="Box plot of MSE of batch gradient descent")

mean(MSE_batch)
sd(MSE_batch)
IQR(MSE_batch)
```

We can see that the average MSE is 0.3202527292. But we have an outlier (see the box plot). Let us eliminate the outlier and find the mean and standard deviation:


```{r}
MSE_batch_sorted <- sort(MSE_batch)
mean(MSE_batch_sorted[-length(MSE_batch_sorted)])
sd(MSE_batch_sorted[-length(MSE_batch_sorted)])
MSE_batch_sorted
```

If we ignore the outlier, then the range of MSE will be from 0.1192821 and 0.5029043.

###Cross validation of Gradient Descent (stochastic method)

Let us cross validate the Stochastic gradient method also, and see what range of MSE we obtain.

```{r}
i<-1
j<-5

l <- nrow(df_std)
MSE_sto <- vector()
k <- 1
repeat{

if(j > l) break

include <- seq(i,j,1)
exclude <- -1*include
sg <- stochastic_gradient_descent(0.01,df_std[exclude,])
 
MSE_sto[k] <- sum((df_std[include,]$price - (sg$theta_0+sg$theta_1*df_std[include,]$sft+sg$theta_2*df_std[include,]$beds))^2)/5
k<- (k+1)

i <- i+5
j<- j+5

}
```


```{r}
plot(MSE_sto,type="l",xlab="CV iteration",main="Cross Validation of stochastic gradient descent")
boxplot(MSE_sto,main="Box plot of MSE of stochastic gradient descent")
mean(MSE_sto)
sd(MSE_sto)
IQR(MSE_sto)
```

We can see that the average MSE is 0.3233594. But we have an outlier (see the box plot). Let us eliminate the outlier and get the range of MSE again:

```{r}
MSE_sto_sorted <- sort(MSE_sto)
mean(MSE_sto_sorted[-length(MSE_sto_sorted)])
sd(MSE_sto_sorted[-length(MSE_sto_sorted)])
MSE_sto_sorted
```
So if we eliminate the outlier, then our MSE ranges between 0.11176045 to 0.4992942. These values are approximately equal to the batch method's CV MSE range.

###Let us plot the batch gradient MSE and Stochastic MSE together in a single plot:

```{r}
plot(MSE_batch,type="l",col="red",ylab="MSE",xlab="CV Iteration",main="Batch MSE and Stochastic MSE comparision")
par(new=TRUE)
plot(MSE_sto,type="l",col="blue",ylab="MSE",xlab="CV Iteration")
legend('topright', c('Batch MSE', 'Stochastic MSE'),lty=1,col=c("red","blue"),bty='n',cex=.75) 
   
```

The graph clearly shows that the MSE is almost the same for both Batch Gradient descent and Stochastic MSE. So for the prediction of house prices we can safely use Stochastic gradient descent, and a linear model.


                                       ~~~**End of Assignment**~~~